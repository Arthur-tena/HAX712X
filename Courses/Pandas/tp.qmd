---
title: "Pandas"
format:
  html:
      out.width: 50%

---

This lecture is extracted and adapted from the [Pandas tutorial](https://github.com/jorisvandenbossche/pandas-tutorial/blob/master/01-pandas_introduction.ipynb) by Joris Van den Bossche.

For R users, you might also want to read [Pandas: Comparison with R / R libraries](https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html) for a smooth start in Pandas.

```{python}
%matplotlib inline
import os
import numpy as np
import calendar
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pooch  # download data / avoid re-downloading
from IPython import get_ipython


sns.set_palette("colorblind")
pd.options.display.max_rows = 8
```

## Dataset 1: Titanic dataset

First, it is important to download automatically remote files for reproducibility (and avoid typing names manually)
```{python}
url = "http://josephsalmon.eu/enseignement/datasets/titanic.csv"
path_target = "./titanic.csv"
path, fname = os.path.split(path_target)
pooch.retrieve(url, path=path, fname=fname, known_hash=None)  # if needed `pip install pooch`
```

Reading the file as a `pandas` dataframe:

```{python}
df_titanic_raw = pd.read_csv("titanic.csv")
```

Visualize the end of the dataset:
```{python}
df_titanic_raw.tail(n=3)
```

Visualize the beginning of the dataset:

```{python}
df_titanic_raw.head(n=5)
```



### Missing values
It is common to encounter features/covariates with missing values.
In `pandas` they were mostly handled as `np.nan` (not a number).
In the future, they will be treated as `NA` (Not Available), in a similar way as in R; see the [Pandas documentation on missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html) for standard behavior and details.

Note that the main difference between `pd.NA` and `np.nan` is that `pd.NA` propagates even for comparisons:

```{python}
pd.NA == 1
```
whereas
```{python}
np.nan == 1
```

Testing the presence of missing values
```{python}
pd.isna(pd.NA)
pd.isna(np.nan)
```

The simplest strategy (when you can / when you have enough samples) consists of removing all nans/NAs.


```{python}
df_titanic = df_titanic_raw.dropna()
df_titanic.tail(3)
```

```{python}
# Useful info on the dataset (especially missing values!)
df_titanic.info()
```

```{python}
# Check that the `Cabin` information is mostly missing; the same hold for `Age`
df_titanic_raw.info()
```

### Description of the `titanic.csv` dataset

Details of the dataset are given [here](https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3info.txt)

- `Survived`: 	 Survival 	0 = No, 1 = Yes
- `Pclass`: 	 Ticket class 	1 = 1st, 2 = 2nd, 3 = 3rd
- `Sex`: 	     Sex male/female
- `Age`: 	     Age in years
- `Sibsp`: 	     # of siblings/spouses aboard the Titanic
- `Parch`: 	     # of parents/children aboard the Titanic
- `Ticket`: 	 Ticket number
- `Fare`: 	     Passenger fare
- `Cabin`: 	     Cabin number
- `Embarked`:    Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton
- `Name`:        Name of the passenger
- `PassengerId`: Number to identify passenger

::: {.callout-note}

For those interested, an extended version of the dataset is available here <https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic.txt>.
:::

### Simple descriptive statistics
```{python}
df_titanic.describe()
```

### Visualization

- Histograms (please avoid...often useless)
```{python}
#| layout-ncol: 1

fig, ax = plt.subplots(1, 1, figsize=(5, 5))
ax.hist(df_titanic['Age'], density=True, bins=25)
plt.xlabel('Age')
plt.ylabel('Proportion')
plt.title("Passager age histogram")
plt.show()
```

- Kernel Density Estimate (KDE):  :

```{python}
#| layout-ncol: 1

fig, ax = plt.subplots(1, 1, figsize=(5, 5))
sns.kdeplot(
    df_titanic["Age"], ax=ax, fill=True, cut=0, bw_adjust=0.1
)
plt.xlabel("Proportion")
plt.ylabel("Age")
plt.title("Passager age kernel density estimate")
plt.tight_layout()
plt.show()
```

::: {.callout-note}

Note: the bandwidth parameter (here encoded as `bw_adjust`) controls the smoothing level. It is a common parameter in kernel smoothing, investigated thoroughly in the non-parametric statistics literature.
:::

- Swarmplot:
```{python}
#| layout-ncol: 1

fig, ax = plt.subplots(1, 1, figsize=(5, 5))
sns.swarmplot(
    data=df_titanic_raw,
    ax=ax,
    x="Sex",
    y="Age",
    hue="Survived",
    palette={0: "r", 1: "k"},
    order=["female", "male"],
)
plt.title("Passager age by gender/survival")
plt.legend(labels=["Died", "Survived"], loc="upper left")
plt.tight_layout()
plt.show()

```


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE density over histogram

Plot the density estimate over the histogram
:::

### Widgets
Interactive interaction with codes and output is nowadays easier and easier (see also Shiny app in R-software).
In Python, one can use `widgets` and the `interact` package for this purpose.
We are going to visualize that on the simple KDE and histogram examples.

### XXX -> to pyplot

```python
def hist_explore(
    dataset=df_titanic,
    variable=df_titanic.columns,
    n_bins=24,
    alpha=0.25,
    density=False,
):
    fig, ax = plt.subplots(1, 1, figsize=(5, 5))
    ax.hist(
        dataset[variable], density=density, bins=n_bins, alpha=alpha
    )  # standardization
    plt.ylabel("Density level")
    plt.title(f"Dataset {dataset.attrs['name']}:\n Histogram for passengers' age")
    plt.tight_layout()
    plt.show()


interact(
    hist_explore,
    dataset=fixed(df_titanic),
    n_bins=(1, 50, 1),
    alpha=(0, 1, 0.1),
    density=False,
)
```


```python
def kde_explore(dataset=df_titanic, variable=df_titanic.columns, bw=5):
    fig, ax = plt.subplots(1, 1, figsize=(5, 5))
    sns.kdeplot(dataset[variable], bw_adjust=bw, shade=True, cut=0, ax=ax)
    plt.ylabel("Density level")
    plt.title(f"Dataset {dataset.attrs['name']}:\n KDE for passengers'  {variable}")
    plt.tight_layout()
    plt.show()

interact(kde_explore, dataset=fixed(df_titanic), bw=(0.001, 2, 0.01))

```

### `Groupby` function
How does the survival rate change w.r.t. to sex?

```{python}
df_titanic_raw.groupby('Sex')[['Survived']].aggregate(lambda x: x.mean())
```

How does the survival rate change w.r.t. the class?


```{python}
df_titanic.columns
```


```{python}
#| layout-ncol: 1

fig, ax = plt.subplots(1, 1, figsize=(5, 5))

df_titanic.groupby('Pclass')['Survived'].aggregate(lambda x:
                                                   x.mean()).plot(ax=ax,kind='bar')
plt.xlabel('Classe')
plt.ylabel('Taux de survie')
plt.title('Taux de survie par classe')
plt.show()
```


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE median by class

Perform a similar analysis with the median for the price per class in pounds.
:::


### `catplot`: a visual `groupby`

```{python}
#| layout-ncol: 1

ax=sns.catplot(
    x="Pclass",
    y="Age",
    hue="Sex",
    palette={'female': 'red', 'male': 'b'},
    data=df_titanic_raw,
    jitter = '0.2',
    s=8,
)
sns.move_legend(ax, "upper left", bbox_to_anchor=(0.8, 0.8))
plt.show()
```

```{python}
#| layout-ncol: 1

ax=sns.catplot(
    x="Pclass",
    y="Age",
    hue="Sex",
    palette={'female': 'red', 'male': 'b'},
    alpha=0.8,
    data=df_titanic_raw,
    kind='swarm',
    s=11,
)
sns.move_legend(ax, "upper left", bbox_to_anchor=(0.8, 0.8))
plt.show()
```

```{python}
#| layout-ncol: 1

ax=sns.catplot(
    x="Sex",
    y="Age",
    hue="Sex",
    palette={'female': 'red', 'male': 'b'},
    col='Pclass',
    alpha=0.8,
    data=df_titanic_raw,
    kind='swarm',
    s=6,
    height=5,
    aspect=0.35
)
plt.show()
```

```{python}
#| layout-ncol: 1

ax=sns.catplot(x='Pclass',
            y='Age',
            hue="Sex",
            palette={'female': 'red', 'male': 'b'},
            data=df_titanic_raw,
            kind="violin",
            alpha=0.8,
)
sns.move_legend(ax, "upper left", bbox_to_anchor=(0.8, 0.8))
plt.show()
```

Beware: large difference in sex ratio by class

```{python}
df_titanic_raw.groupby(['Sex', 'Pclass'])[['Sex']].count()
df_titanic_raw.groupby(['Sex'])[['Sex']].count()
```


Consider checking the raw data, as often boxplots or simple statistics are not enough to understand the diversity inside the data; see for instance the discussion here: <https://sigmoid.social/@ct_bergstrom@fediscience.org/110267907203021857>

```{python}

References:

- [Practical Business Python](https://pbpython.com/groupby-agg.html), Comprehensive Guide to Grouping and Aggregating with Pandas,  by Chris Moffitt

### pd.crosstab

```{python}
pd.crosstab(
    df_titanic_raw["Sex"],
    df_titanic_raw["Pclass"],
    values=df_titanic_raw["Sex"],
    aggfunc="count",
    normalize=False,
)
```


```{python}
df_titanic
```

```{python}
df_titanic.index
```

```{python}
df_titanic.columns
```

```{python}
pd.options.display.max_rows = 12
df_titanic.dtypes

df_titanic['Name'].astype(str)
```


### Extract `numpy` arrays from dataframes
useful for using packages on top of `pandas` (*e.g.,* `sklearn`, though nowadays it works out of the box with `pandas`)

```{python}
array_titanic = df_titanic.values  # associated numpy array
array_titanic
```

::: {.callout-important appearance='default' icon="false"}
## EXERCISE: `dropna`

Perform the following operation: remove the columns Cabin from the raw
dataset, and then remove the rows with the variable `Age` missing.

Hint: check the ['dropna' documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).
:::


### 1D dataset: Series (a column of a DataFrame)

A Series is a labeled 1D column of a kind.

```{python}
fare = df_titanic['Fare']
fare
```


### Attributes *Series*: indices and values

```{python}
fare.values[:10]
```

Contrarily to *numpy* arrays, you can index with other formats than integers:

```{python}
# Be careful, what follows changes the indexing
df_titanic_raw = df_titanic_raw.set_index('Name')
df_titanic_raw
```

```{python}
age = df_titanic_raw['Age']
age['Behr, Mr. Karl Howell']
```

```{python}
age.mean()
```

```{python}
df_titanic_raw[age < 2]
```

```{python}
# You can come back to the original indexing
df_titanic_raw = df_titanic_raw.reset_index()
```


# Counting values for categorical variables

```{python}
df_titanic_raw['Embarked'].value_counts(normalize=False, sort=True,
                                        ascending=False)
```

```{python}
pd.options.display.max_rows = 70
df_titanic[df_titanic['Embarked'] == 'C']
```
Comments: not all passengers from Cherbourg are Gallic (&#127467;&#127479;: *gaulois*) ...


What is the survival rate for raw data?
```{python}
df_titanic_raw['Survived'].mean()
```

What is the survival rate for data after removing missing values?
```{python}
df_titanic['Survived'].mean()
```
See also the command:

```{python}
df_titanic.groupby(['Sex'])[['Survived', 'Age', 'Fare']].mean()
```

Conclusion: Be careful when you remove some missing values, the missingness might be informative!


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: More data analysis

What was the proportion of women on the boat?
:::


### Data import/export

The Pandas library supports many formats:

 - CSV, text
 - SQL database
 - Excel
 - HDF5
 - JSON
 - HTML
 - pickle
 - sas, stata
 - ...

### Exploration

```{python}
pd.options.display.max_rows = 8
df_titanic_raw.tail()
```

```{python}
df_titanic_raw.head()
```


### Access values by line/columns etc.

- `iloc`
```{python}
df_titanic_raw.iloc[0:2, 1:8]
```

- `loc`

```{python}
# with original index:
# df_titanic_raw.loc[128]

# with naming indexing
df_titanic_raw = df_titanic_raw.set_index('Name')  # You can only do it once !!
df_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Fare']
```

```{python}
df_titanic_raw.loc['Bonnell, Miss. Elizabeth']
```

```{python}
df_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived']
df_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived'] = 0
```

```{python}
df_titanic_raw.loc['Bonnell, Miss. Elizabeth']
```

```{python}
# set back the original value
df_titanic_raw.loc['Bonnell, Miss. Elizabeth', 'Survived'] = 1
df_titanic_raw = df_titanic_raw.reset_index()  # Back to original indexing
```

### `groupby`

```{python}
df_titanic.groupby(['Sex'])[['Survived', 'Age', 'Fare', 'Pclass']].mean()
```

### Create binned values

```{python}
#| layout-ncol: 1
bins=np.arange(0, 100, 10)
current_palette = sns.color_palette()

df_test = pd.DataFrame({ 'Age': pd.cut(df_titanic['Age'], bins=bins, right=False)})
ax = sns.countplot(data=df_test, x='Age', color=current_palette[0])
ax.tick_params(axis='x', labelrotation=90)
```


### Second Case study: Air quality in Paris (Source: Airparif)

```{python}
url = "http://josephsalmon.eu/enseignement/datasets/20080421_20160927-PA13_auto.csv"
path_target = "./20080421_20160927-PA13_auto.csv"
path, fname = os.path.split(path_target)
pooch.retrieve(url, path=path, fname=fname, known_hash=None)
```

You can run for instance in a terminal:
```{python}
#| eval: false
!head -26 ./20080421_20160927-PA13_auto.csv
```

Alternatively:

```{python}
#| eval: false
from IPython import get_ipython
get_ipython().system('head -26 ./20080421_20160927-PA13_auto.csv')
```

**References**:

- [Working with time series](https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html), Python Data Science Handbook by Jake VanderPlas

```{python}
polution_df = pd.read_csv('20080421_20160927-PA13_auto.csv', sep=';',
                          comment='#',
                          na_values="n/d",
                          converters={'heure': str})
```

```{python}
pd.options.display.max_rows = 30
polution_df.head(25)
```

### Data preprocessing


```{python}
# check types
polution_df.dtypes

# check all
polution_df.info()
```

For more info on the nature of Pandas objects, see this discussion on [Stackoverflow](https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object).
Moreover, things are slowly moving from `numpy` to `pyarrow`, cf. [Pandas user guide](https://pandas.pydata.org/docs/user_guide/pyarrow.html)

::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: handling missing values

What is the meaning of "na_values="n/d" above?
Note that an alternative can be obtained with the command
`polution_df.replace('n/d', np.nan, inplace=True)`

:::

### Issues with non-conventional hours/day format

Start by changing to integer type (e.g., `int8`):
```{python}
polution_df['heure'] = polution_df['heure'].astype(np.int8)
polution_df['heure']
```
No data is from 1 to 24... not conventional so let's make it from 0 to 23
```{python}
polution_df['heure'] = polution_df['heure'] - 1
polution_df['heure']
```
and back to strings:

```{python}
polution_df['heure'] = polution_df['heure'].astype('str')
polution_df['heure']
```

### Time processing

Note that we have used the following conventions:
 - d = day
 - m=month
 - Y=year
 - H=hour
 - M=minutes

```{python}
time_improved = pd.to_datetime(polution_df['date'] +
                               ' ' + polution_df['heure'] + ':00',
                               format='%d/%m/%Y %H:%M')

time_improved
```


```{python}
polution_df['date'] + ' ' + polution_df['heure'] + ':00'
```

Create correct timing format in the dataframe

```{python}
polution_df['DateTime'] = time_improved
 # remove useless columns:
del polution_df['heure']
del polution_df['date']
polution_df
```

Visualize the data set now that the time is well formatted:

```{python}
polution_ts = polution_df.set_index(['DateTime'])
polution_ts = polution_ts.sort_index(ascending=True)
polution_ts.head(12)
```


```{python}
polution_ts.describe()
```

```{python}
#| layout-ncol: 1

fig, axes = plt.subplots(2, 1, figsize=(6, 6), sharex=True)

axes[0].plot(polution_ts['O3'])
axes[0].set_title("Ozone polution: daily average in Paris")
axes[0].set_ylabel("Concentration (µg/m³)")

axes[1].plot(polution_ts['NO2'])
axes[1].set_title("Nitrogen polution: daily average in Paris")
axes[1].set_ylabel("Concentration (µg/m³)")
plt.show()
```

```{python}
#| layout-ncol: 1

fig, axes = plt.subplots(2, 1, figsize=(10, 5), sharex=True)

axes[0].plot(polution_ts['O3'].resample('d').max(), '--')
axes[0].plot(polution_ts['O3'].resample('d').min(),'-.')

axes[0].set_title("Ozone polution: daily average in Paris")
axes[0].set_ylabel("Concentration (µg/m³)")

axes[1].plot(polution_ts['NO2'].resample('d').max(),  '--')
axes[1].plot(polution_ts['NO2'].resample('d').min(),  '-.')

axes[1].set_title("Nitrogen polution: daily average in Paris")
axes[1].set_ylabel("Concentration (µg/m³)")

plt.show()
```
Source: <https://www.tutorialspoint.com/python/time_strptime.htm>

::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: extreme values per day </font>
 Provide the same plots as before, but with daily best and worst on the same
 figures (and use different colors and/or styles)

Q: Is the pollution getting better over the years or not?
:::

```{python}
#| layout-ncol: 1

fig, ax = plt.subplots(1, 1)
polution_ts['2008':].resample('Y').mean().plot(ax=ax)
# Sample by year (A pour Annual) or Y for Year
plt.ylim(0, 50)
plt.title("Pollution evolution: \n yearly average in Paris")
plt.ylabel("Concentration (µg/m³)")
plt.xlabel("Year")
plt.show()
```

Loading colors:
```{python}
#| layout-ncol: 1
sns.set_palette("GnBu_d", n_colors=7)
polution_ts['weekday'] = polution_ts.index.weekday  # Monday=0, Sunday=6
polution_ts['weekend'] = polution_ts['weekday'].isin([5, 6])

polution_week_no2 = polution_ts.groupby(['weekday', polution_ts.index.hour])[
    'NO2'].mean().unstack(level=0)
polution_week_03 = polution_ts.groupby(['weekday', polution_ts.index.hour])[
    'O3'].mean().unstack(level=0)
plt.show()
```


```{python}
#| layout-ncol: 1

fig, axes = plt.subplots(2, 1, figsize=(7, 7), sharex=True)

polution_week_no2.plot(ax=axes[0])
axes[0].set_ylabel("Concentration (µg/m³)")
axes[0].set_xlabel("Intraday evolution")
axes[0].set_title(
    "Daily NO2 concentration: weekend effect?")
axes[0].set_xticks(np.arange(0, 24))
axes[0].set_xticklabels(np.arange(0, 24), rotation=45)
axes[0].set_ylim(0, 60)

polution_week_03.plot(ax=axes[1])
axes[1].set_ylabel("Concentration (µg/m³)")
axes[1].set_xlabel("Intraday evolution")
axes[1].set_title("Daily O3 concentration: weekend effect?")
axes[1].set_xticks(np.arange(0, 24))
axes[1].set_xticklabels(np.arange(0, 24), rotation=45)
axes[1].set_ylim(0, 70)
axes[0].legend().set_visible(False)
# ax.legend()
axes[1].legend(labels=[day for day in calendar.day_name], loc='lower left', bbox_to_anchor=(1, 0.1))

plt.tight_layout()
plt.show()
```



```{python}
polution_ts['month'] = polution_ts.index.month  # Janvier=0, .... Decembre=11
polution_ts['month'] = polution_ts['month'].apply(lambda x:
                                                  calendar.month_abbr[x])
polution_ts.head()
```


```{python}
polution_month_no2 = polution_ts.groupby(['month', polution_ts.index.hour])[
    'NO2'].mean().unstack(level=0)
polution_month_03 = polution_ts.groupby(['month', polution_ts.index.hour])[
    'O3'].mean().unstack(level=0)
```


```{python}
#| layout-ncol: 1

sns.set_palette("Paired", n_colors=12)

fig, axes = plt.subplots(2, 1, figsize=(7, 7), sharex=True)

polution_month_no2.plot(ax=axes[0])
axes[0].set_ylabel("Concentration (µg/m³)")
axes[0].set_xlabel("Hour of the day")
axes[0].set_title(
    "Daily profile per month (NO2): weekend effect?")
axes[0].set_xticks(np.arange(0, 24))
axes[0].set_xticklabels(np.arange(0, 24), rotation=45)
axes[0].set_ylim(0, 90)

polution_month_03.plot(ax=axes[1])
axes[1].set_ylabel("Concentration (µg/m³)")
axes[1].set_xlabel("Heure de la journée")
axes[1].set_title("Daily profile per month (O3): weekend effect?")
axes[1].set_xticks(np.arange(0, 24))
axes[1].set_xticklabels(np.arange(0, 24), rotation=45)
axes[1].set_ylim(0, 90)
axes[0].legend().set_visible(False)
# ax.legend()
axes[1].legend(labels=calendar.month_name[1:], loc='lower left',
               bbox_to_anchor=(1, 0.1))
plt.tight_layout()
plt.show()
```


## Third example: explore a dataset on bike accidents in France
**References**:

- [Data original source](https://www.data.gouv.fr/fr/datasets/accidents-de-velo-en-france/)
- [Possible visualization](https://koumoul.com/en/datasets/accidents-velos)


```{python}
url = "https://koumoul.com/s/data-fair/api/v1/datasets/accidents-velos/raw"
path_target = "./bicycle_db.csv"
path, fname = os.path.split(path_target)
pooch.retrieve(url, path=path, fname=fname, known_hash=None)
```



```{python}
# df: data frame
df_bikes = pd.read_csv("bicycle_db.csv", na_values="", low_memory=False,
                       dtype={'data': str, 'heure': str, 'departement': str})
```

In June 2023, the author decided to change the name of the columns, hence we had to define a dictionary to come back to legacy names:

```{python}

new2old = {
"hrmn": "heure",
"secuexist": "existence securite",
"grav": "gravite accident",
"dep": "departement"
}

df_bikes.rename(columns=new2old, inplace=True)

```

```{python}
#| eval: false
get_ipython().system('head -5 ./bicycle_db.csv')
```

```{python}
pd.options.display.max_columns = 40
df_bikes.head()
```

```{python}
df_bikes['existence securite'].unique()
```


```{python}
df_bikes['gravite accident'].unique()
```

### Handle missing values in `heure`

```{python}
df_bikes['date'].hasnans
df_bikes['heure'].hasnans
```

```{python}
pd.options.display.max_rows = 20
df_bikes.iloc[400:402]
```

Remove missing hours cases by `np.nan`:
```{python}
df_bikes['heure'] = df_bikes['heure'].replace('', np.nan)
df_bikes.iloc[400:402]
```


```{python}
df_bikes.dropna(subset=['heure'], inplace=True)
df_bikes.iloc[399:402]
```


::: {.callout-important appearance='default' icon="false"}
## EXERCISE: start/end of the study

Can you find the starting day and the ending day of the study automatically?

Hint: Sort the data! You can sort the data by time for instance, say with `df.sort('Time')`.

```{python}
df_bikes['date'] + ' ' + df_bikes['heure']
```


```{python}
# ADAPT OLD to create the df_bikes['Time']

time_improved = pd.to_datetime(df_bikes['date'] +
                               ' ' + df_bikes['heure'],
                               format='%Y-%m-%d %H',
                               errors='coerce')

# Where d = day, m=month, Y=year, H=hour, M=minutes
```

```{python}
df_bikes['Time'] = time_improved
# remove rows with NaT
df_bikes.dropna(subset=["Time"], inplace=True)
# set new index 
df_bikes.set_index('Time', inplace=True)
# remove useless columns
df_bikes.drop(columns=['heure', 'date'], inplace=True)
```

```{python}
df_bikes.info()
```

```{python}
df_bike2 = df_bikes.loc[
    :, ["gravite accident", "existence securite", "age", "sexe"]
]
df_bike2["existence securite"].replace({"Inconnu": np.nan}, inplace=True)
df_bike2.dropna(inplace=True)

```


::: {.callout-important appearance='default' icon="false"}
## EXERCISE: Is the helmet saving your life?

Perform an analysis so that you can check the benefit or not of wearing a
helmet to save your life.
Beware: Preprocessing is needed to use `pd.crosstab`, `pivot_table` to avoid
issues.
:::
```{python}
#| echo: false

group = df_bike2.pivot_table(columns='existence securite',
                             index=['gravite accident', 'sexe'],
                             aggfunc={'age': 'count'}, margins=True)
group
```


```{python}
#| echo: false

pd.crosstab(df_bike2['existence securite'],
            df_bike2['gravite accident'], normalize='index') * 100
```

```{python}
#| echo: false

pd.crosstab(df_bike2['existence securite'],
            df_bike2['gravite accident'], values=df_bike2['age'],
            aggfunc='count', normalize='index') * 100
```

::: {.callout-important appearance='default' icon="false"}
## EXERCISE:  Are men and women dying equally on a bike?

Perform an analysis to check differences between men's and women's survival.
:::

```{python}
#| echo: false

idx_dead = df_bikes['gravite accident'] == '3 - Tué'
df_deads = df_bikes[idx_dead]
df_gravite = df_deads.groupby('sexe').size() / idx_dead.sum()
df_gravite
```


```{python}
#| echo: false

df_bikes.groupby('sexe').size()  / df_bikes.shape[0]
```

```{python}
#| echo: false

pd.crosstab(df_bike2['sexe'],
            df_bike2['gravite accident'],
            values=df_bike2['age'], aggfunc='count',
            normalize='columns', margins=True) * 100
```


### To conclude
Note that in the dataset, the information on the level of bike practice by gender is missing.


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: Accident during the week?

Perform an analysis to check when the accidents are occurring during the week.
:::

```{python}
df_bikes
```


```{python}
#| layout-ncol: 1
# Chargement des couleurs
sns.set_palette("GnBu_d", n_colors=7)

df_bikes['weekday'] = df_bikes.index.day_of_week  # Monday=0, Sunday=6

accidents_week = df_bikes.groupby(['weekday', df_bikes.index.hour])[
    'sexe'].count().unstack(level=0)

fig, axes = plt.subplots(1, 1, figsize=(7, 7))
accidents_week.plot(ax=axes)
axes.set_ylabel("Accidents")
axes.set_xlabel("Heure de la journée")
axes.set_title(
    "Profil journalier des accidents: effet du weekend?")
axes.set_xticks(np.arange(0, 24))
axes.set_xticklabels(np.arange(0, 24), rotation=45)
axes.legend(
    labels=[day for day in calendar.day_name],
    loc='upper left',
    )
plt.tight_layout()
plt.show()
```


```{python}
df_bikes.groupby(['weekday', df_bikes.index.hour])[
    'sexe'].count()
```


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: Accident during the year

Perform an analysis to check when the accidents are occurring during the week.
:::

```{python}
#| layout-ncol: 1

df_bikes['month'] = df_bikes.index.month  # Janvier=0, .... Decembre=11
df_bikes['month'] = df_bikes['month'].apply(lambda x: calendar.month_abbr[x])
df_bikes.head()

sns.set_palette("GnBu_d", n_colors=12)  # sns.set_palette("colorblind",...)

df_bikes_month = df_bikes.groupby(['month', df_bikes.index.hour])[
    'age'].count().unstack(level=0)

fig, axes = plt.subplots(1, 1, figsize=(7, 7), sharex=True)

df_bikes_month.plot(ax=axes)
axes.set_ylabel("Concentration (µg/m³)")
axes.set_xlabel("Heure de la journée")
axes.set_title(
    "Profil journalier de la pollution au NO2: effet du weekend?")
axes.set_xticks(np.arange(0, 24))
axes.set_xticklabels(np.arange(0, 24), rotation=45)
axes.legend(labels=calendar.month_name[1:], loc='upper left')

plt.tight_layout()
plt.show()
```


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: Accidents by department
Perform an analysis to check when the accidents are occurring for each department, relative to population size.
:::

```{python}
path_target = "./dpt_population.csv"
url = "https://public.opendatasoft.com/explore/dataset/population-francaise-par-departement-2018/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B"
path, fname = os.path.split(path_target)
pooch.retrieve(url, path=path, fname=fname, known_hash=None)
```

```{python}
df_dtp_pop = pd.read_csv("dpt_population.csv", sep=";", low_memory=False)

df_dtp_pop['Code Département'].replace('2A', '20A',inplace=True)
df_dtp_pop['Code Département'].replace('2B', '20B',inplace=True)
df_dtp_pop.sort_values(by=['Code Département'], inplace=True)

df_bikes['departement'].replace('2A', '20A',inplace=True)
df_bikes['departement'].replace('2B', '20B',inplace=True)
df_bikes.sort_values(by=['departement'], inplace=True)
# Clean extra departements
df_bikes = df_bikes.loc[df_bikes['departement'].isin(df_dtp_pop['Code Département'].unique())]

gd = df_bikes.groupby(['departement'], as_index=True, sort=True).size()

data = {'code': gd.index,
        '# Accidents per million': gd.values}
df = pd.DataFrame(data)
df['# Accidents per million'] = df['# Accidents per million'].values * 10000./ df_dtp_pop['Population'].values
```

```{python}
path_target = "./departements-avec-outre-mer.geojson"
url = "https://raw.githubusercontent.com/gregoiredavid/france-geojson/master/departements-avec-outre-mer.geojson"
path, fname = os.path.split(path_target)
pooch.retrieve(url, path=path, fname=fname, known_hash=None)
```


```{python}
#| layout-ncol: 1
import plotly.express as px
import geopandas

departement = geopandas.read_file('departements-avec-outre-mer.geojson')
departement['code'].replace('2A', '20A', inplace=True)
departement['code'].replace('2B', '20B', inplace=True)

departement.sort_values(by=['code'], inplace=True)

a = ['0'+ str(i) for i in range(1, 10)]
b = [str(i) for i in range(1, 10)]
dict_replace = dict(zip(a, b))

departement['code'].replace(dict_replace, inplace=True)
df['code'].replace(dict_replace, inplace=True)

departement['code'].replace('20A', '2A', inplace=True)
departement['code'].replace('20B', '2B', inplace=True)
df['code'].replace('20A', '2A',inplace=True)
df['code'].replace('20B', '2B',inplace=True)

departement.set_index('code', inplace=True)

fig = px.choropleth_mapbox(
    df,
    geojson=departement,
    locations="code",
    color="# Accidents per million",
    range_color=(0, df['# Accidents per million'].max()),
    color_continuous_scale="rdbu",
    center={'lat': 47, 'lon': 2},
    zoom=3.25,
    mapbox_style="white-bg",
)
fig.update_traces(colorbar_orientation='h', selector=dict(type='choroplethmapbox'))
fig.update_layout(
    title_text = 'Accidents per million inhabitants by department',
)
fig.layout.coloraxis.colorbar.thickness = 20
fig.layout.coloraxis.colorbar.orientation = 'h'
fig.layout.coloraxis.colorbar.y = -0.2
fig.layout.coloraxis.colorbar.x = 0.2

fig.show()
```


::: {.callout-important appearance='default' icon="false"}
##  EXERCISE: Accidents by department
Perform an analysis to check when the accidents are occurring for each department, relative to the area of the *departements*.
:::

```{python}
#| echo: false
#| eval: false

path_target = "./dpt_area.csv"
url = "https://www.regions-et-departements.fr/fichiers/departements-francais.csv"
path, fname = os.path.split(path_target)
pooch.retrieve(url, path=path, fname=fname, known_hash=None)
```


**References**:

- Other interactive tools for data visualization: Altair, Bokeh. See comparisons by Aarron Geller: [link](https://sites.northwestern.edu/researchcomputing/2022/02/03/what-is-the-best-interactive-plotting-package-in-python/)

- An interesting tutorial: [Altair introduction](https://infovis.fh-potsdam.de/tutorials/)

- [Introduction on geopandas](https://geopandas.org/en/stable/getting_started/introduction.html)

- [Notebook on French departememen/womennts issues](https://www.kaggle.com/code/scratchpad/notebook670ec82922/edit)

- [Choropleth Maps in practice with Plotly and Python](https://towardsdatascience.com/choropleth-maps-in-practice-with-plotly-and-python-672a5eef3a19) by Thibaud Lamothe
- [European data on France geography](https://data.europa.eu/data/datasets/5fb11a6b13b6030b71898d44?locale=fr)

<!--
fr_chart = pygal.maps.fr.Departments(human_readable=True)
display = "ratio_accident"


if display == "ratio_accident":
    fr_chart.title = 'Accidents by departement'
    gd = df_bikes.groupby(['departement']).size()
    gd = (gd / df_dtp_pop['Population'])  # mean accident per habitant
else:
    fr_chart.title = 'Deaths by departement'
    df_deads = df_bikes[df_bikes['gravite accident']=='3 - Tué']
    df_gravite = df_deads.groupby('departement').size()
    # gd = df_bikes.groupby(['departement']).aggregate(lambda: x->sum(x))
    gd = (df_gravite / df_dtp_pop['Population'])  # mean deaths per habitant

# Area normalization
normalization = True
if normalization == True:
    gd = (gd / df_dtp_area['SUPERFICIE (km²)'])
gd.dropna(inplace=True)   # anoying NA due to 1 vs 01 in datasets
fr_chart.add('Accidents', gd.to_dict())
# fr_chart.render()
# fr_chart.render_to_file('./chatr.svg')  # Write the chart in a specified file
```
--->
